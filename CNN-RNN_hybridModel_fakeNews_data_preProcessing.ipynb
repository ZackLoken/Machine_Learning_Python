{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://github.com/Shubha23/Fake-News-Detection-Text-Preprocessing-and-Classification/blob/master/fake-news-detection-text-pre-processing-using-nltk.ipynb\n",
    "# and: https://github.com/manthan89-py/Fake_News_detection/blob/master/Fake%20News.ipynb\n",
    "\n",
    "# Original data can be found here: https://www.uvic.ca/ecs/ece/isot/datasets/fake-news/index.php\n",
    "\n",
    "# Import necessary Python libraries, modules, etc.\n",
    "import time # for generating timestamps \n",
    "import re # for regular expressions\n",
    "import string as st # for removing punctuation\n",
    "import numpy as np # for linear algebra\n",
    "import pandas as pd # for frame processing\n",
    "import matplotlib.pyplot as plt # for data visualization\n",
    "import nltk # for natural language processing\n",
    "from nltk.corpus import stopwords # for removing english stopwords\n",
    "from nltk.stem import WordNetLemmatizer # for term stemming\n",
    "import sklearn # for predictive data analysis\n",
    "from sklearn import preprocessing # for data preprocessing\n",
    "from sklearn.model_selection import train_test_split # for splitting data into test/train sets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # for text vectorization\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from IPython.core.interactiveshell import InteractiveShell # to modify Jupyter notebook configuration\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # so that all outputs in a cell are returned (instead of last instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fake and true news CSVs into Pandas dataframes\n",
    "true_news = pd.read_csv('True.csv') \n",
    "fake_news = pd.read_csv('Fake.csv')\n",
    "\n",
    "# Add column for fake/true label\n",
    "true_news['label'] = 'true'\n",
    "fake_news['label'] = 'fake'\n",
    "\n",
    "# True and Fake news value counts - are they balanced?\n",
    "print(\"Compare number of observations in true news and fake news data frames\")\n",
    "true_news['label'].value_counts()\n",
    "print()\n",
    "fake_news['label'].value_counts()\n",
    "print()\n",
    "\n",
    "# Remove random rows from fake_news (n = 2064) data frame so it has same number of rows as true_news\n",
    "np.random.seed(5)\n",
    "\n",
    "remove_n = 2064\n",
    "drop_indices = np.random.choice(fake_news.index, remove_n, replace = False)\n",
    "fake_news = fake_news.drop(drop_indices)\n",
    "\n",
    "# Check that have same number of observations now\n",
    "print(\"True and fake datasets should have same number of samples now...\")\n",
    "true_news['label'].value_counts()\n",
    "print()\n",
    "fake_news['label'].value_counts()\n",
    "print()\n",
    "\n",
    "# Preview first 5 rows in datasets to ensure they imported properly\n",
    "print(\"Preview of the raw datasets to ensure they imported properly:\")\n",
    "true_news.head()\n",
    "print()\n",
    "fake_news.head()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine true_news and fake_news data frames into one\n",
    "dfs = [true_news, fake_news]\n",
    "news_data = pd.concat(dfs)\n",
    "\n",
    "# Instantiate instance of LabelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Assign numerical values to column of target values (true = 1, fake = 0)\n",
    "news_data['target'] = le.fit_transform(news_data['label'])\n",
    "\n",
    "# Concatenate text columns and isolate only relevant columns for analysis (i.e., text and target)\n",
    "news_data['text'] = news_data['title'] + ' ' + news_data['text']\n",
    "news_data = news_data[['text', 'target']]\n",
    "\n",
    "# Check that binary values were assigned correctly\n",
    "print(\"Dimensions of data frame that will be cleaned:\")\n",
    "news_data.shape # data frame dimensions\n",
    "print()\n",
    "\n",
    "print(\"First and last five rows of pre-cleaned concatenated dataset:\")\n",
    "news_data.head(-5) # first 5 and last 5 rows\n",
    "print()\n",
    "\n",
    "print(\"Null values by column:\")\n",
    "news_data.isnull().sum() # check for null values\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate WordNetLemmatizer() -- reduce words to their roots\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Download multilingual Wordnet data from OMW\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# List of english stopwords\n",
    "nltk.download('stopwords') \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Download english dictionary ('wordnet')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for cleaning data\n",
    "def data_cleaning(row):\n",
    "    row = row.lower() # convert text into lowercase\n",
    "    row = re.sub('[^a-zA-Z]', ' ', row) # remove number and special characters using regex (keep words only)\n",
    "    token = row.split() # split the data into tokens\n",
    "    news = [wnl.lemmatize(word) for word in token if not word in stop_words] # lemmatize the words and remove any stopwords (e.g., a, an, the, etc.)\n",
    "    row_clean = [word for word in news if len(word) >= 3] # only keep words greater than or equal to length of 3\n",
    "    cleaned_news = ' '.join(row_clean) # join all tokenized words with space in between \n",
    "    \n",
    "    return cleaned_news\n",
    "\n",
    "# Clean the data - might take a couple minutes to run.\n",
    "news_data['text'] = news_data['text'].apply(lambda x : data_cleaning(x)) # 'text' column\n",
    "print(\"First and last five rows after cleaning the data:\")\n",
    "news_data.head(-5) # check that cleaning went as planned\n",
    "print()\n",
    "\n",
    "# Check for null values\n",
    "print(\"Null values by column:\")\n",
    "news_data.isnull().sum() # want zero null values\n",
    "print()\n",
    "\n",
    "# Check number unique values in each column\n",
    "print(\"Unique values by column:\")\n",
    "news_data.nunique() # number unique values in each column\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate news_data into predictor and response variables\n",
    "X = news_data.iloc[:, 0] # features used to predict if news is fake or true\n",
    "y = news_data.iloc[:, 1] # what we're trying to predict: (whether is fake(0) or true(1))\n",
    "\n",
    "# Split the data into training and test subsets\n",
    "train_data, test_data, train_target, test_target = train_test_split(X, y, random_state = 5, train_size = 0.80)\n",
    "\n",
    "# View first 5 rows to ensure data split worked correctly\n",
    "print(\"The first and last five rows of training_data are:\")\n",
    "train_data.head(-5)\n",
    "print()\n",
    "\n",
    "print(\"The first and last five rows of testing_data are:\")\n",
    "test_data.head(-5)\n",
    "print()\n",
    "\n",
    "print(\"The first and last five rows of training_target are:\")\n",
    "train_target.head(-5)\n",
    "print()\n",
    "\n",
    "# Double check data partitioning after split - are they balanced?\n",
    "print(\"Data partitioning of true and fake values in the training data:\")\n",
    "train_target.value_counts() # balanced partition of train data\n",
    "print()\n",
    "\n",
    "print(\"Data partitioning of true and fake values in the testing data:\")\n",
    "test_target.value_counts() # balanced partition of test data\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of different values to try for TfidVectorizer max_features (i.e., top key words)\n",
    "key_words = [100, 500, 1000, 5000] # How many of the top key words to keep - iterate over list\n",
    "n_grams = [(1, 1), (1, 2), (2, 2), (1, 3), (2, 3), (3, 3)] # ngram_range dictates if we keep 1 word (1, 1), 1 or 2 words (1, 2) etc. \n",
    "\n",
    "# Max features parameter chooses the top n words (iterate over key_words list)\n",
    "for ng in n_grams:\n",
    "\n",
    "    # ngram_range parameter defines phrase length.\n",
    "    # (1, 1) keeps single words only, (3,3) keeps three word phrases,\n",
    "    # (1, 3) keeps one, two, or three word phrases in top n words and so on. \n",
    "    for kw in key_words:\n",
    "\n",
    "        # Create variable for storing start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Instantiate vectorizer\n",
    "        vectorizer = TfidfVectorizer(max_features = kw, lowercase = False, ngram_range = ng)\n",
    "\n",
    "        # Vectorize training data\n",
    "        vec_train_data = vectorizer.fit_transform(train_data)\n",
    "        vec_train_data = vec_train_data.toarray()\n",
    "\n",
    "        # Vectorize test data\n",
    "        vec_test_data = vectorizer.transform(test_data).toarray()\n",
    "\n",
    "        # Double check dimensions\n",
    "        print(f\"The pre-vectorization training data shape using top {kw} words and an n-gram range of {ng} is: {train_data.shape}\")\n",
    "        print()\n",
    "        print(f\"The pre-vectorization testing data shape using top {kw} words and an n-gram range of {ng} is: {test_data.shape}\")\n",
    "        print()\n",
    "        print(f\"The post-vectorization training data shape using top {kw} words and an n-gram range of {ng} is: {vec_train_data.shape}\")\n",
    "        print()\n",
    "        print(f\"The post-vectorization testing data shape using top {kw} words and an n-gram range of {ng} is: {vec_test_data.shape}\")\n",
    "        print()\n",
    "        \n",
    "        # Store vectorized training and test data into respective dfs - this is the final data to use in training/evaluation\n",
    "        training_data = pd.DataFrame(vec_train_data , columns = vectorizer.get_feature_names_out())\n",
    "        print(f\"The first and last five rows of final training data using top {kw} words and an n-gram range of {ng} are:\") \n",
    "        training_data.head(-5)\n",
    "        print()\n",
    "\n",
    "        testing_data = pd.DataFrame(vec_test_data , columns = vectorizer.get_feature_names_out())\n",
    "        print(f\"The first and last five rows of final testing data using top {kw} words and an n-gram range of {ng} are:\") \n",
    "        testing_data.head(-5)\n",
    "        print()\n",
    "\n",
    "        # Print time stamp for each iteration in nested loop - can be modified to calculate runtime for each classifier\n",
    "        current_time = time.time() # current time in executed code\n",
    "        elapsed_time = current_time - start_time # elapsed time (i.e., how long it took to run iteration)\n",
    "        print(f\"The time to vectorize the data using top {kw} words and an n-gram range of {ng} is: {elapsed_time:.2f} seconds\")\n",
    "        print()\n",
    "\n",
    "        # # Write final vectorized training and testing data to CSV - if you want them saved.\n",
    "        # # Took me about 45 minutes to run when saving individually as CSVs. \n",
    "        # training_data.to_csv(f'training_data_{kw}words_{ng}range.csv')\n",
    "        # testing_data.to_csv(f'testing_data_{kw}words_{ng}range.csv')\n",
    "\n",
    "        # Continue here with your evaluation of traditional classification model inside the for loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word cloud of top 500 words for presentation\n",
    "text = \" \".join([x for x in news_data['text']])\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (30, 20)\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(\n",
    "    max_words = 500,\n",
    "    width = 3000,\n",
    "    height = 2000,\n",
    "    background_color = 'white',\n",
    "    stopwords = stop_words).generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
